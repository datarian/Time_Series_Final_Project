\clearpage

# Introduction {#introduction}
Dendrochronology, i.e. the method to date tree rings, is used in several fields of science. One relevant application of the method is in archeology where the method is used to date (wooden) artifacts. In climate science, dated tree ring widths helps to learn more about the climate of the past. Tree ring chronologies also helps to backdate the radiocarbon concentration, which is used in radiocarbon calibration - another method for dating artifacts.

Usually, time series in dendrochnology span several hundred years. Such long time series are constructed by merging and overlaying shorter lasting time series from individual trees. By taking the mean of yearly ring widths, a single time series is finally built. Chronologies are generally constructed for defined geographical regions as the environment (i.e the local climate, soil properties and terrain) has substantial impact on the growth of trees [@baillie2012slice].  

From the perspective of statistics, tree ring data are interesting because they define a time series. According to @woollons1990time and @FOX2001261 AR(1) and ARMA(1,1) are popular choices for the time series process after having removed the trend. AR(2) is seen as an appropriate choice as well [see e.g. @dplR]. 

In this report we focus on imputing one missing value on an otherwise know time series. Our contribution to the literature is twofold. Firstly, by investigating a Swiss dendrochronology from the Bernese Oberland we give further evidence on the stochastic process of dendrochronological time series. Secondly, although dendrochronologies span hundreds or thousands of years without gaps these days, it is still important and useful to check the correctness of a chronology. Observations that are far away from what we expect, i.e. from the imputed value, may indicate that the observation is not very plausible. This paper gives a first insight how well imputations work. 

<!-- https://www.environmentalscience.org/dendrochronology-tree-rings-tell-us -->
<!-- https://c14.arch.ox.ac.uk/calibration.html -->

# Methods {#methods}
Classical time series analysis relies on stationarity. Non-stationary time series have thus to be transformed such that the resulting series is stationary. In section 2.1 we describe three common approaches proposed in the (dendrochronology) literature. We also present our method of choice that worked best on our dataset. In section 2.2 we discuss how we can find the best model. Section 2.3 is dedicated to a short introduction about Kalman smoothing and how the method can be used to impute missing values. We conclude the section by showing how we can transform back to the original time series, since we are interested on imputations on the original time series.   

## Transformations {#transformations}
Popular methods to transform non-stationary dendrochronologies are the Box-Cox transformation, the Power transformation, and the method of Warren (1980). Additionally, we investigated an 'intuitive' approach. The different methods are explained now.

The Box-Cox transformation aims to stabilize the variance by finding the transformation parameter $\lambda$ that minimizes the coefficient of variation of the series. The time series is either transformed to $Y_t^{(\lambda)} = ((Y_t + c)^{\lambda}-1)/\lambda$, for a $\lambda$ unequal to 0, or $Y_t^{(\lambda)}=ln(Y_t + c)$, when $\lambda = 0$ [@box1976time]. 

A similar approach to the Box-Cox transformation is the Power transformation proposed by @guerrero2004 where $\lambda$ is estimated in a model-independent manner, i.e. irrespective of the time series process that is deduced from the resulting stationary series. In this method a time series is divided into $H\geq2$ equal-sized subseries, for which then the mean $\bar{Z_h}$ and the standard deviation $S_h$ is computed. The optimal parameter $\lambda$ is chosen such that $S_h/\bar{Z}^{1-\lambda}_h=c$, with $h=1,...H$ holds for a constant value $c>0$.

The method of Warren (1980) (cited in @woollons1990time) proposes to initially detrend the data by fitting a polynomial of the form $Y = \alpha t^{\beta} e^{\delta t}$ and then to divide the residuals by the fitted values. Taking the $ln$ on both siedes gives $ln(Y) = ln(\alpha) + \beta ln(t) + \delta t$. This approach is convenient, because the parameters can be estimated by a linear model.

An additional method was deduced on a more intuitive approach. Having raw data at hand, we were able to also take into consideration the standard deviation of yearly ring width measurements ${sd}_t$. We then proceed by:

1. Scaling the means by dividing them by the standard deviation: $B_t = \bar{r}_t/sd_t$, $t=1,..., T$,
2. Log tranform the scaled means $B_t$ to get $C_t = ln(B_t)$, 
3. Finally, fit a linear model of the form: $X_t = \beta_0 + \beta_1 C_t$

We have not seen this method in literature. A drawback  is that it requires to estimate the standard deviation at times $t=1,...,T$, which is not possible if only data on the yearly mean tree ring width is at hand.

## Model fitting {#modelfitting}
The stochastic process of a time series can either be found visually by inspecting the ACF and PACF or computationally by comparing the AIC or a similar criterium. We used both approaches. Table \ref{tab:acfpacfdecision} shows the decision matrix that is considered to identify the form of the process visually. Automatic model selection can be done for example with the `autofit` function from the `itsmr` package and the `auto.arima` function from the `forecast` package in R.

```{r acfpacfdecision}
apdec <- data.frame(rbind(c("exponential decrease", "p spikes"),
                       c("q spikes", "exponential decrease"),
                       c("exponential decrease", "exponential decrease")),
                    row.names = c("AR(p)","MA(q)","ARMA(p,q)"))
colnames(apdec) = c("ACF","PACF")
kable(apdec,booktabs=T,
      caption="Decision matrix for model identification from ACF and PACF plots.") %>%
    kable_styling(latex_options = c("hold_position"), position = "center")
```

The candidate models are further tested by analyzing the residuals graphically. Precisely, the standardized residuals, the autocorrelation function (ACF) of the resudials and the p-values of the Ljung-Box test applied on the residuals are studied.

## Imputations {#imputationsI}
As we would like to use as much information as possible, a good method to impute missing values is Kalman smoothing [WIESO?]. In order to apply Kalman smoothing, we have to formulate possibly appropriate models to describe the process of our time series of interest in state space form. Motivated from the literature and from our analysis as well, we will assume AR(1), AR(2) and ARMA(1,1) processes as reasonable and compare their performance with respect to imputations.

State space models are usually applied in situations where an observed value $y$ is an (affine) function of a state variable $x$ and some noise. To assume Gaussian noise for the state equation and the observation equation and moreover a (prior) Gaussian distribution for the initial state allows to calculate the distribution of the state at any time $t$ within the series. Kalman smoothing works perfectly with missing values, but a potential drawback might be that it relies on Gaussian distributions.^[However, this is also the case when a time series model is fitted by maximum likelihood.] 

For the analysis, AR(1), AR(2) and ARMA(1,1) are formulated in state space form as in @Hyndman, and the parameters are estimated with the `dlm` package in R [@petris2009dlm]. Since an estimate of the standard deviation is required for the backtransformation of the series (see section 2.1), an estimate for the standard deviation is required. We will see later in the report that the standard deviation can be estimated quite well by assuming an ARMA(1,1) model. By defining $X_t$ as the transformed time series, the original series $Y_t$ is obtained by: 

1. adding the linear trend back, i.e. $\tilde{X_t} = X_t + \alpha + \beta t$;
2. exponentiating $\tilde{X_t}$, i.e. $\hat{X_t} = exp(\tilde{X_t})$,
3. and multiplying the standard deviation to $\hat{X_t}$, i.e. $Y = \hat{X_t} sd_t$ 

We also check our implementation with the `na.kalman` method of the `imputeTS` package of @imputeTS. Furthermore, all models are compared to the benchmark, which is to simply linearly interpolate the original time series. 

