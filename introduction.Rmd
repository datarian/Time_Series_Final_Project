\clearpage

# Introduction {#introduction}
Dendrochronology, i.e. the method to date tree rings, is used in several fields of science. One relevant application of the method is in archeology where it is used to date (wooden) artifacts. In climate science, dated tree ring widths helps to learn more about the climate of the past. Tree ring chronologies also helps to backdate the radiocarbon concentration, which is used in radiocarbon calibration -- another method for dating artifacts.

Usually, time series in dendrochnology span several hundred years. Such long series are constructed by merging and overlaying shorter lasting time series from individual trees. By taking the mean of yearly ring widths, a single time series is built. Chronologies are generally constructed for defined geographical regions as the environment (i.e the local climate, soil properties and terrain) has substantial impact on the growth of trees [@baillie2012slice].  

From the perspective of statistics, tree ring data are of interest because they define a time series. According to @woollons1990time and @FOX2001261 AR(1) and ARMA(1,1) are popular choices for the time series process after having removed the trend. AR(2) is seen as an appropriate choice as well [see e.g. @dplR]. 

In this report we focus on imputing one missing value on an otherwise known time series. Our contribution to the literature is twofold. Firstly, by investigating a Swiss dendrochronology from the Bernese Oberland we give further evidence on the stochastic process of dendrochronological time series. Secondly, although dendrochronologies span hundreds or thousands of years without gaps these days, it is still important and useful to check the correctness of a chronology. One way to check the correctness is for instance to compare the observed values with what we expect from the (estimated) process of the time series. Observations that are far away from the expectation, so-called outliers, may need further attention. This paper gives a first insight how well imputations work in dendrochronology.

<!-- indicate that the observation is not very plausible.   -->

<!-- https://www.environmentalscience.org/dendrochronology-tree-rings-tell-us -->
<!-- https://c14.arch.ox.ac.uk/calibration.html -->

# Methods {#methods}
Classical time series analysis relies on stationarity. Non-stationary time series have to be transformed such that the resulting series is stationary. In [Section 2.1](#transformations) we describe three common approaches proposed in the (dendrochronology) literature. We also present our method of choice that worked best on our dataset. In [Section 2.2](#modelfitting) we discuss how the best model can be found. [Section 2.3](#imputationsI) is dedicated to a short introduction about Kalman smoothing and how the method can be used to impute missing values. since we are interested on imputations on the original time series, we conclude the section by showing how to backtransform the stationary time series.   

## Transformations {#transformations}
Popular methods to transform non-stationary dendrochronologies are the Box-Cox transformation, the Power transformation, and the method of Warren (1980). Additionally, we investigate an 'intuitive' approach that is not reported in the literature. The different methods are explained below.

The Box-Cox transformation aims to stabilize the variance by finding the transformation parameter $\lambda$ that minimizes the coefficient of variation of the series. The time series is either transformed to $Y_t^{(\lambda)} = ((Y_t + c)^{\lambda}-1)/\lambda$, for a $\lambda$ unequal to 0, or $Y_t^{(\lambda)}=ln(Y_t + c)$, in case of $\lambda = 0$ [@box1976time]. A similar approach to the Box-Cox transformation is the Power transformation proposed by @guerrero2004 where $\lambda$ is estimated in a model-independent manner, i.e. irrespective of the time series process, that is deduced from the resulting stationary series. In this method a time series is divided into $H\geq2$ equal-sized subseries, for which then the mean $\bar{Z_h}$ and the standard deviation $S_h$ is computed. The optimal parameter $\lambda$ is chosen such that $S_h/\bar{Z}^{1-\lambda}_h=c$, with $h=1,...H$ holds for a constant value $c>0$. 

The method of Warren (1980) (cited in @woollons1990time) proposes to initially detrend the data by fitting a polynomial of the form $Y = \alpha t^{\beta} e^{\delta t}$ and then to divide the residuals by the fitted values. Taking the $ln$ on both sides gives $ln(Y) = ln(\alpha) + \beta ln(t) + \delta t$. This approach is convenient, because the parameters can be estimated by a linear model. 

An additional method is deduced on a more intuitive approach. Having raw data at hand, we are able to also take into consideration the standard deviation of yearly ring width measurements ${sd}_t$ at time $t=1,...,T$. In particular, we proceed by:

1. Scaling the means by dividing them by the standard deviation: $B_t = \bar{r}_t/sd_t$, $t=1,..., T$,
2. Log tranform the scaled means $B_t$ to get $C_t = ln(B_t)$, 
3. Finally, fit a linear model of the form $C_t = \beta_0 + \beta_1 t$ to obtain the residuals $X_t$

A drawback of the last method is that it requires to estimate the standard deviation at time $t=1,...,T$, which is not possible if only data on aggregated yearly mean tree ring widths are at hand.

## Model fitting {#modelfitting}
The stochastic process of a time series can be either found visually by inspecting the ACF (autocorrelation function) and PACF (partial autocorrelation function) or computationally by comparing the AIC or a similar criterium. We use both approaches. Table \ref{tab:acfpacfdecision} shows the decision matrix, which is considered to identify the form of the process visually. Automatic model selection can be done in R for instance with the `autofit` function from the `itsmr` package and the `auto.arima` function from the `forecast` package.

```{r acfpacfdecision}
apdec <- data.frame(rbind(c("exponential decrease", "p spikes"),
                       c("q spikes", "exponential decrease"),
                       c("exponential decrease", "exponential decrease")),
                    row.names = c("AR(p)","MA(q)","ARMA(p,q)"))
colnames(apdec) = c("ACF","PACF")
kable(apdec,booktabs=T,
      caption="Decision matrix for model identification from ACF and PACF plots.") %>%
    kable_styling(latex_options = c("hold_position"), position = "center")
```

The candidate models are further tested by analyzing the residuals graphically. Precisely, the standardized residuals, the ACF of the resudials and the p-values of the Ljung-Box test applied on the residuals are studied.

## Imputations {#imputationsI}
As we would like to use as much information as possible, a good method to impute missing data is Kalman smoothing, since it allows to use the information of the observed values before and after the missing value. In order to apply Kalman smoothing, we have to formulate appropriate models, which we think describe the process of our time series of interest well, in state space form. Motivated from the literature and from our analysis as well, we assume AR(1), AR(2) and ARMA(1,1) as reasonable processes and compare their performance with respect to imputations.

State space models are usually applied to situations where an observed value $y$ is an (affine) function of a state variable $x$ and some noise. Assuming Gaussian noise for the state and the observation equation and a (prior) Gaussian distribution for the initial state as well, allows to calculate the distribution of the state at any time $t$ within the series. Kalman smoothing works perfectly with missing values, but a potential drawback of the method is it reliance on Gaussian distributions.^[However, this is also the case when a time series model is fitted by maximum likelihood.] 

For the analysis, AR(1), AR(2) and ARMA(1,1) are formulated in state space form as in @Hyndman, and the parameters are estimated with the `dlm` package in R [@petris2009dlm]. Since an estimate of the standard deviation is required for the backtransformation of the series (see [Section 2.1](#transformations)), an estimate for the standard deviation is required too. We will see later in the report that the standard deviation can be estimated quite well by assuming an ARMA(1,1) model. By defining $X_t$ as the transformed time series, the original series $Y_t$ is obtained by: 

1. adding the linear trend back, i.e. $\tilde{X_t} = X_t + \alpha + \beta t$,
2. exponentiating $\tilde{X_t}$, i.e. $\hat{X_t} = exp(\tilde{X_t})$,
3. and multiplying the standard deviation to $\hat{X_t}$, i.e. $Y = \hat{X_t} sd_t$ 

We also check our implementation with the `na.kalman` method of the `imputeTS` package of @imputeTS. Furthermore, all models are compared to simple linear interpolation of the original time series, which is our benchmark. 

